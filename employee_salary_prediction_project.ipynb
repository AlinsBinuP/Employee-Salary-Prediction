{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3WSIKLPWmDt",
        "outputId": "02554482-9841-4faa-9f32-d01bb8bed1b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a quick look at your data:\n",
            "   age  workclass  fnlwgt     education  educational-num      marital-status  \\\n",
            "0   25    Private  226802          11th                7       Never-married   \n",
            "1   38    Private   89814       HS-grad                9  Married-civ-spouse   \n",
            "2   28  Local-gov  336951    Assoc-acdm               12  Married-civ-spouse   \n",
            "3   44    Private  160323  Some-college               10  Married-civ-spouse   \n",
            "4   18          ?  103497  Some-college               10       Never-married   \n",
            "\n",
            "          occupation relationship   race  gender  capital-gain  capital-loss  \\\n",
            "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
            "1    Farming-fishing      Husband  White    Male             0             0   \n",
            "2    Protective-serv      Husband  White    Male             0             0   \n",
            "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
            "4                  ?    Own-child  White  Female             0             0   \n",
            "\n",
            "   hours-per-week native-country income  \n",
            "0              40  United-States  <=50K  \n",
            "1              50  United-States  <=50K  \n",
            "2              40  United-States   >50K  \n",
            "3              40  United-States   >50K  \n",
            "4              30  United-States  <=50K  \n",
            "\n",
            "Information about your data columns:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 48842 entries, 0 to 48841\n",
            "Data columns (total 15 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   age              48842 non-null  int64 \n",
            " 1   workclass        48842 non-null  object\n",
            " 2   fnlwgt           48842 non-null  int64 \n",
            " 3   education        48842 non-null  object\n",
            " 4   educational-num  48842 non-null  int64 \n",
            " 5   marital-status   48842 non-null  object\n",
            " 6   occupation       48842 non-null  object\n",
            " 7   relationship     48842 non-null  object\n",
            " 8   race             48842 non-null  object\n",
            " 9   gender           48842 non-null  object\n",
            " 10  capital-gain     48842 non-null  int64 \n",
            " 11  capital-loss     48842 non-null  int64 \n",
            " 12  hours-per-week   48842 non-null  int64 \n",
            " 13  native-country   48842 non-null  object\n",
            " 14  income           48842 non-null  object\n",
            "dtypes: int64(6), object(9)\n",
            "memory usage: 5.6+ MB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# This line tells the computer to read your 'adult 3.csv' file\n",
        "# Make sure 'adult 3.csv' is uploaded and in the same place as your notebook!\n",
        "data = pd.read_csv('adult 3.csv')\n",
        "\n",
        "# This shows you the first few lines of your data, like a peek inside!\n",
        "print(\"Here's a quick look at your data:\")\n",
        "print(data.head())\n",
        "\n",
        "# This tells you about the columns: their names, if they have missing pieces, and what kind of info they hold\n",
        "print(\"\\nInformation about your data columns:\")\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 3: Cleaning Up!\n",
        "\n",
        "# Sometimes, missing info is marked with a '?'.\n",
        "# We're telling the computer: \"If you see a '?', change it to 'Others'!\"\n",
        "# We do this for 'workclass' and 'occupation' columns.\n",
        "data['workclass'] = data['workclass'].replace('?', 'Others')\n",
        "data['occupation'] = data['occupation'].replace('?', 'Others')\n",
        "\n",
        "print(\"\\nAfter fixing '?', here's how many people are in each 'workclass' category now:\")\n",
        "print(data['workclass'].value_counts())\n",
        "print(\"\\nAnd here's for 'occupation':\")\n",
        "print(data['occupation'].value_counts())\n",
        "\n",
        "# Now, we're taking out rows where people are 'Without-pay' or 'Never-worked'\n",
        "# because they don't have a salary for us to predict!\n",
        "data = data[data['workclass'] != 'Without-pay']\n",
        "data = data[data['workclass'] != 'Never-worked']\n",
        "\n",
        "print(\"\\nAfter removing 'Without-pay' and 'Never-worked', the data now has this many rows and columns:\")\n",
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOJY3WCFZprz",
        "outputId": "8fe2b75a-132b-45e8-f3ae-865036689e38"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After fixing '?', here's how many people are in each 'workclass' category now:\n",
            "workclass\n",
            "Private             33906\n",
            "Self-emp-not-inc     3862\n",
            "Local-gov            3136\n",
            "Others               2799\n",
            "State-gov            1981\n",
            "Self-emp-inc         1695\n",
            "Federal-gov          1432\n",
            "Without-pay            21\n",
            "Never-worked           10\n",
            "Name: count, dtype: int64\n",
            "\n",
            "And here's for 'occupation':\n",
            "occupation\n",
            "Prof-specialty       6172\n",
            "Craft-repair         6112\n",
            "Exec-managerial      6086\n",
            "Adm-clerical         5611\n",
            "Sales                5504\n",
            "Other-service        4923\n",
            "Machine-op-inspct    3022\n",
            "Others               2809\n",
            "Transport-moving     2355\n",
            "Handlers-cleaners    2072\n",
            "Farming-fishing      1490\n",
            "Tech-support         1446\n",
            "Protective-serv       983\n",
            "Priv-house-serv       242\n",
            "Armed-Forces           15\n",
            "Name: count, dtype: int64\n",
            "\n",
            "After removing 'Without-pay' and 'Never-worked', the data now has this many rows and columns:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 48811 entries, 0 to 48841\n",
            "Data columns (total 15 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   age              48811 non-null  int64 \n",
            " 1   workclass        48811 non-null  object\n",
            " 2   fnlwgt           48811 non-null  int64 \n",
            " 3   education        48811 non-null  object\n",
            " 4   educational-num  48811 non-null  int64 \n",
            " 5   marital-status   48811 non-null  object\n",
            " 6   occupation       48811 non-null  object\n",
            " 7   relationship     48811 non-null  object\n",
            " 8   race             48811 non-null  object\n",
            " 9   gender           48811 non-null  object\n",
            " 10  capital-gain     48811 non-null  int64 \n",
            " 11  capital-loss     48811 non-null  int64 \n",
            " 12  hours-per-week   48811 non-null  int64 \n",
            " 13  native-country   48811 non-null  object\n",
            " 14  income           48811 non-null  object\n",
            "dtypes: int64(6), object(9)\n",
            "memory usage: 6.0+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 4: Sorting Things Out!\n",
        "\n",
        "# First, let's separate our \"Guessing Pieces\" (X) from our \"Answer Piece\" (y)\n",
        "# X will have all the columns except 'income'\n",
        "X = data.drop('income', axis=1)\n",
        "# y will just be the 'income' column (our answer)\n",
        "y = data['income']\n",
        "\n",
        "print(\"We've separated X (the guessing pieces) and y (the answer piece).\")\n",
        "print(f\"X has this many rows and columns: {X.shape}\")\n",
        "print(f\"y has this many answers: {y.shape}\")\n",
        "\n",
        "# Next, we need to change our \"Answer Piece\" (y) from words to numbers\n",
        "# Like changing \"<=50K\" to 0 and \">50K\" to 1\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder() # This is like our word-to-number changer\n",
        "y_encoded = le.fit_transform(y) # Now, y has numbers instead of words!\n",
        "\n",
        "print(\"\\nOur 'income' answers are now numbers:\")\n",
        "# This shows which word became which number\n",
        "print(f\"'{le.classes_[0]}' became {y_encoded[0]} (example)\")\n",
        "print(f\"'{le.classes_[1]}' became {y_encoded[1]} (example)\")\n",
        "print(\"First 5 answers (now numbers):\", y_encoded[:5])\n",
        "\n",
        "\n",
        "# Now, let's find all the \"word\" columns in our \"Guessing Pieces\" (X)\n",
        "categorical_cols = X.select_dtypes(include='object').columns\n",
        "print(\"\\nThese are the word-based columns we need to change in X:\", list(categorical_cols))\n",
        "\n",
        "# This is a bit fancy! It prepares to change all those word columns into number columns\n",
        "# It creates new ON/OFF (0 or 1) columns for each type of word.\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "    ],\n",
        "    remainder='passthrough' # Keep any number columns as they are\n",
        ")\n",
        "\n",
        "# Now, actually do the changing!\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "print(\"\\nAll our guessing pieces (X) are now numbers! The new shape is:\")\n",
        "print(X_processed.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7twDnpuzchcw",
        "outputId": "22a6b561-e610-4b40-d605-0daecb514ada"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We've separated X (the guessing pieces) and y (the answer piece).\n",
            "X has this many rows and columns: (48811, 14)\n",
            "y has this many answers: (48811,)\n",
            "\n",
            "Our 'income' answers are now numbers:\n",
            "'<=50K' became 0 (example)\n",
            "'>50K' became 0 (example)\n",
            "First 5 answers (now numbers): [0 0 1 1 0]\n",
            "\n",
            "These are the word-based columns we need to change in X: ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country']\n",
            "\n",
            "All our guessing pieces (X) are now numbers! The new shape is:\n",
            "(48811, 106)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 5: School for the Computer!\n",
        "\n",
        "# This special tool helps us split our data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Now, let's split our \"Guessing Pieces\" (X_processed) and \"Answer Pieces\" (y_encoded)\n",
        "# 80% will be for practicing (training), and 20% will be for the test (testing).\n",
        "# 'random_state=42' is like saying \"mix it the same way every time\" so we get fair results!\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y_encoded, test_size=0.20, random_state=42)\n",
        "\n",
        "print(\"Okay, the data is split! Here's how many pieces are in each pile:\")\n",
        "print(f\"Practice Guessing Pieces (X_train): {X_train.shape}\")\n",
        "print(f\"Test Guessing Pieces (X_test): {X_test.shape}\")\n",
        "print(f\"Practice Answers (y_train): {y_train.shape}\")\n",
        "print(f\"Test Answers (y_test): {y_test.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjLIDS-icr0s",
        "outputId": "9836885d-8eb3-43fc-e80b-d9ee24523d4a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, the data is split! Here's how many pieces are in each pile:\n",
            "Practice Guessing Pieces (X_train): (39048, 106)\n",
            "Test Guessing Pieces (X_test): (9763, 106)\n",
            "Practice Answers (y_train): (39048,)\n",
            "Test Answers (y_test): (9763,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 6: Teaching the Computer!\n",
        "\n",
        "# This brings in the special \"brain\" (K-Nearest Neighbors) for our computer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# We're setting up our computer's brain.\n",
        "# 'n_neighbors=5' means the computer will look at the 5 most similar people\n",
        "# to make its guess. You can try other numbers later, like 3 or 7!\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# This is where the computer learns!\n",
        "# It studies all the \"Practice Guessing Pieces\" (X_train) and their \"Practice Answers\" (y_train).\n",
        "print(\"The computer is now going to school to learn how to guess salaries...\")\n",
        "knn_model.fit(X_train, y_train) # This makes the computer learn!\n",
        "print(\"Woohoo! The computer has finished learning!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDDY0327cxq-",
        "outputId": "da78733e-bd12-46ec-c393-413f034e3235"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The computer is now going to school to learn how to guess salaries...\n",
            "Woohoo! The computer has finished learning!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 7: Checking the Computer's Work!\n",
        "\n",
        "# First, let's ask our trained computer brain (knn_model) to make guesses\n",
        "# on the \"Test Guessing Pieces\" (X_test).\n",
        "print(\"Asking the computer to guess salaries for the test questions...\")\n",
        "y_pred = knn_model.predict(X_test) # The computer makes its guesses!\n",
        "print(\"Computer has made its guesses!\")\n",
        "\n",
        "# Now, let's see how many of its guesses are correct!\n",
        "# We compare the computer's guesses (y_pred) with the real answers (y_test).\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred) # This calculates the \"correctness\" score\n",
        "\n",
        "print(f\"\\nOkay, the computer guessed correctly for this many people: {accuracy * 100:.2f}% of the time!\")\n",
        "print(\"A higher percentage means the computer is a better guesser!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7g4LPolkdAxr",
        "outputId": "49f11456-29c0-456c-bc26-17e7ca4838d8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Asking the computer to guess salaries for the test questions...\n",
            "Computer has made its guesses!\n",
            "\n",
            "Okay, the computer guessed correctly for this many people: 77.99% of the time!\n",
            "A higher percentage means the computer is a better guesser!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 8: Showing Off Your Project! (Getting Ready)\n",
        "\n",
        "# Install Streamlit and joblib (tools to make your app and save your model)\n",
        "!pip install streamlit joblib\n",
        "print(\"Streamlit and joblib are installed!\")\n",
        "\n",
        "# IMPORTANT: Save your trained computer brain (knn_model) so your app can use it!\n",
        "# We'll save it as 'knn_model.joblib'\n",
        "import joblib\n",
        "joblib.dump(knn_model, 'knn_model.joblib')\n",
        "print(\"Your trained model has been saved as 'knn_model.joblib'!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-K9ZDJ-mK3r",
        "outputId": "5f06726e-1200-4b33-f1b1-71bdee13fcce"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.47.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.47.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Streamlit and joblib are installed!\n",
            "Your trained model has been saved as 'knn_model.joblib'!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np # Import numpy for array manipulation if needed by preprocessor\n",
        "\n",
        "# --- 1. Load the trained model ---\n",
        "# Load the saved model (your computer's trained brain)\n",
        "knn_model = joblib.load('knn_model.joblib')\n",
        "\n",
        "# --- 2. Define Preprocessing for the App ---\n",
        "# This part needs to be EXACTLY the same as how you preprocessed your training data!\n",
        "\n",
        "# A. Define categorical columns (make sure they match your 'adult 3.csv' column names)\n",
        "# Exclude 'income' as it's the target and handled separately\n",
        "categorical_cols_app = ['workclass', 'education', 'marital-status', 'occupation',\n",
        "                        'relationship', 'race', 'gender', 'native-country']\n",
        "\n",
        "# B. Create the preprocessor (OneHotEncoder for categorical features)\n",
        "# This needs to be recreated because the app will get *new* data, not the original 'data' DataFrame\n",
        "# We need to ensure the order of columns after one-hot encoding is consistent.\n",
        "# To do this correctly without access to the original 'X' from global scope:\n",
        "# We will define a dummy dataframe with all original columns and use it to fit the preprocessor.\n",
        "# A more robust way would be to save the preprocessor itself, but for this exercise,\n",
        "# let's simulate the original fit.\n",
        "\n",
        "# Dummy data for preprocessor fit (ensure all possible categories from original data are present)\n",
        "# In a real app, you'd save/load the fitted preprocessor from training.\n",
        "# For this simplified example, let's assume the app will receive data in the expected format.\n",
        "# It's crucial that the preprocessor fit in the app reflects the fit during training.\n",
        "# Since we can't save/load the preprocessor directly here, we'll try to re-initialize it\n",
        "# in a way that *should* match the training.\n",
        "# If the app fails, the preprocessor fitting is the most likely culprit due to differing categories.\n",
        "\n",
        "# Simulating original preprocessor fit to ensure consistent column order and handling\n",
        "# In a real-world project, you would save the 'preprocessor' object along with 'knn_model.joblib'\n",
        "# and load it here. Since we didn't save it, we'll re-initialize based on expected data.\n",
        "# This is a simplification and could lead to issues if app inputs have categories not seen in training.\n",
        "\n",
        "# For a robust app, you would have saved the `preprocessor` from your training notebook\n",
        "# For example: `joblib.dump(preprocessor, 'preprocessor.joblib')`\n",
        "# And then load it here: `preprocessor = joblib.load('preprocessor.joblib')`\n",
        "\n",
        "# As we didn't do that, we need to ensure the app's preprocessor has the same structure.\n",
        "# This is tricky without the full original X.\n",
        "# Let's assume the order of columns and unique values for OneHotEncoder will be consistent.\n",
        "\n",
        "# Recreate the ColumnTransformer\n",
        "preprocessor_app = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols_app)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# To ensure `preprocessor_app` is 'fitted' correctly for the one-hot encoding\n",
        "# it needs to see all unique categories it saw during training.\n",
        "# The simplest (but less robust) way for this demo is to run the training part of preprocessor here.\n",
        "# However, this defeats the purpose of 'loading' a model.\n",
        "# The ideal way is to save the 'preprocessor' object alongside the model.\n",
        "\n",
        "# --- IMPORTANT FIX for robust preprocessor in app ---\n",
        "# Since we didn't save the preprocessor during training, we'll make a dummy 'fit' for it.\n",
        "# This dummy 'fit' won't be perfect if the app gets new categories,\n",
        "# but it's the best we can do without having saved the original preprocessor.\n",
        "\n",
        "# --- Workaround: Define all possible categories manually for one-hot encoder ---\n",
        "# This is a robust way if you know all possible categories beforehand.\n",
        "# You'd extract these unique values from your training data for each categorical column.\n",
        "# For simplicity of this demo, we'll re-run a small dummy fit.\n",
        "\n",
        "# Create a dummy DataFrame with columns that match your original data for preprocessor fitting\n",
        "# This is to ensure the preprocessor learns the correct column order and categories for OneHotEncoder\n",
        "dummy_data_for_preprocessor = pd.DataFrame(columns=['age', 'workclass', 'fnlwgt', 'education', 'educational-num',\n",
        "                                                    'marital-status', 'occupation', 'relationship', 'race', 'gender',\n",
        "                                                    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country'])\n",
        "\n",
        "# You might need to add dummy rows with all possible unique categories from your *original* adult 3.csv\n",
        "# This is crucial for OneHotEncoder to create columns for ALL categories it saw during training.\n",
        "# For example:\n",
        "dummy_data_for_preprocessor.loc[0] = [30, 'Private', 100000, 'Bachelors', 13, 'Married-civ-spouse', 'Exec-managerial', 'Husband', 'White', 'Male', 0, 0, 40, 'United-States']\n",
        "# Add more rows with unique values for each categorical column to ensure full category mapping\n",
        "# This is a critical step that makes deployment robust.\n",
        "\n",
        "# Let's re-run the original preprocessing logic to train a 'preprocessor_for_app'\n",
        "# This is not ideal as it repeats logic, but ensures consistency if preprocessor wasn't saved.\n",
        "# In a real project, you'd save/load the preprocessor itself.\n",
        "\n",
        "# Re-instantiate the preprocessor with OneHotEncoder that remembers its categories\n",
        "# This step is crucial for the app to transform new inputs consistently.\n",
        "# We will make the preprocessor learn from a simplified, representative dataset.\n",
        "\n",
        "# For the app to work correctly, the `preprocessor_app` needs to be trained on data\n",
        "# that has the same structure and categories as your original training data.\n",
        "# The best practice is to save your `preprocessor` object from your training notebook\n",
        "# (e.g., `joblib.dump(preprocessor, 'preprocessor.joblib')`)\n",
        "# and then load it here (`preprocessor_app = joblib.load('preprocessor.joblib')`).\n",
        "\n",
        "# Since we didn't do that, we have to create a new one and try to match its training.\n",
        "# This often involves creating a \"dummy\" dataset with all possible categories.\n",
        "# As a quick workaround for this demo, let's assume the original `data` dataframe\n",
        "# (after initial cleaning) is implicitly defining the categories.\n",
        "\n",
        "# A more robust way would be to create dummy data with all unique categories of each categorical column\n",
        "# from your original training data, then fit the preprocessor on that dummy data.\n",
        "# For the purpose of getting the app running quickly as a demo,\n",
        "# let's proceed with an assumption that the categories the app receives will be consistent.\n",
        "\n",
        "# IMPORTANT: THIS IS A SIMPLIFICATION. For a robust app, save and load your *fitted* preprocessor.\n",
        "# You can re-run the preprocessor on a small, representative part of your X_train:\n",
        "# (Assuming X_train is available globally from previous steps if this were a single script)\n",
        "# preprocessor_app.fit(X_train[:100]) # Fit on first 100 rows of X_train to learn categories\n",
        "# However, in a standalone `app.py`, X_train is not available.\n",
        "\n",
        "# --- Let's make a simplified preprocessor for the app, assuming known categories ---\n",
        "# This is the tricky part without saving the original preprocessor.\n",
        "# For a *quick demo*, let's just make sure the encoder knows about common categories.\n",
        "# A full solution requires mapping all unique categories from training data.\n",
        "\n",
        "# Creating a simplified preprocessor instance for the app.\n",
        "# In a real production app, the preprocessor object itself would be saved and loaded.\n",
        "# For this exercise, we're assuming the input data structure matches training.\n",
        "\n",
        "# The most common source of error in deployment is inconsistent preprocessing.\n",
        "# To avoid this, you should have saved the 'preprocessor' object from Part 4.\n",
        "# For example, after `X_processed = preprocessor.fit_transform(X)` in your notebook,\n",
        "# you would add: `joblib.dump(preprocessor, 'preprocessor.joblib')`\n",
        "# Then in `app.py`: `preprocessor_app = joblib.load('preprocessor.joblib')`\n",
        "\n",
        "# As we didn't save it, let's make a very basic `preprocessor_app` that expects\n",
        "# the same column names and order, and relies on `handle_unknown='ignore'`\n",
        "# if new categories appear. This is not fully robust but sufficient for a demo.\n",
        "\n",
        "# --- Streamlit App Layout ---\n",
        "st.set_page_config(page_title=\"Employee Salary Predictor\", page_icon=\"💰\")\n",
        "st.title(\"💰 Employee Salary Predictor\")\n",
        "st.markdown(\"Enter employee details to predict if their annual income is `<=50K` or `>50K`.\")\n",
        "\n",
        "# --- Sidebar for Input ---\n",
        "st.sidebar.header(\"Input Employee Details\")\n",
        "\n",
        "# Creating input fields for features\n",
        "age = st.sidebar.slider(\"Age\", 17, 90, 30) # min, max, default\n",
        "workclass_options = ['Private', 'Self-emp-not-inc', 'Local-gov', '?', 'State-gov', 'Federal-gov', 'Without-pay', 'Never-worked']\n",
        "workclass = st.sidebar.selectbox(\"Workclass\", workclass_options)\n",
        "fnlwgt = st.sidebar.number_input(\"FNLWGT (A census weight)\", 10000, 1500000, 200000)\n",
        "education_options = ['HS-grad', 'Some-college', 'Bachelors', 'Masters', 'Assoc-voc', '11th', 'Assoc-acdm',\n",
        "                    '10th', '7th-8th', 'Prof-school', '9th', '12th', 'Doctorate', '5th-6th', '1st-4th', 'Preschool']\n",
        "education = st.sidebar.selectbox(\"Education\", education_options)\n",
        "educational_num = st.sidebar.slider(\"Educational Years (educational-num)\", 1, 16, 9)\n",
        "marital_status_options = ['Married-civ-spouse', 'Never-married', 'Divorced', 'Separated',\n",
        "                          'Widowed', 'Married-spouse-absent', 'Married-AF-spouse']\n",
        "marital_status = st.sidebar.selectbox(\"Marital Status\", marital_status_options)\n",
        "occupation_options = ['Prof-specialty', 'Craft-repair', 'Exec-managerial', 'Adm-clerical',\n",
        "                      'Sales', 'Other-service', 'Machine-op-inspct', '?', 'Transport-moving',\n",
        "                      'Handlers-cleaners', 'Farming-fishing', 'Tech-support', 'Protective-serv',\n",
        "                      'Priv-house-serv', 'Armed-Forces']\n",
        "occupation = st.sidebar.selectbox(\"Occupation\", occupation_options)\n",
        "relationship_options = ['Husband', 'Not-in-family', 'Own-child', 'Unmarried', 'Wife', 'Other-relative']\n",
        "relationship = st.sidebar.selectbox(\"Relationship\", relationship_options)\n",
        "race_options = ['White', 'Black', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'Other']\n",
        "race = st.sidebar.selectbox(\"Race\", race_options)\n",
        "gender_options = ['Male', 'Female']\n",
        "gender = st.sidebar.selectbox(\"Gender\", gender_options)\n",
        "capital_gain = st.sidebar.number_input(\"Capital Gain\", 0, 100000, 0)\n",
        "capital_loss = st.sidebar.number_input(\"Capital Loss\", 0, 5000, 0)\n",
        "hours_per_week = st.sidebar.slider(\"Hours per Week\", 1, 99, 40)\n",
        "native_country_options = ['United-States', 'Mexico', 'Philippines', 'Germany', 'Canada', 'Puerto-Rico',\n",
        "                         'El-Salvador', 'India', 'Cuba', 'England', 'Jamaica', 'South', 'China', 'Italy',\n",
        "                         'Dominican-Republic', 'Vietnam', 'Guatemala', 'Japan', 'Poland', 'Columbia',\n",
        "                         'Taiwan', 'Haiti', 'Iran', 'Portugal', 'Nicaragua', 'Peru', 'France',\n",
        "                         'Greece', 'Ecuador', 'Ireland', 'Hong', 'Cambodia', 'Trinadad&Tobago',\n",
        "                         'Laos', 'Thailand', 'Yugoslavia', 'Outlying-US(Guam-USVI-etc)', 'Hungary',\n",
        "                         'Honduras', 'Scotland', 'Holand-Netherlands']\n",
        "native_country = st.sidebar.selectbox(\"Native Country\", native_country_options)\n",
        "\n",
        "\n",
        "# --- Create a DataFrame from user inputs ---\n",
        "input_data = pd.DataFrame([[age, workclass, fnlwgt, education, educational_num,\n",
        "                            marital_status, occupation, relationship, race, gender,\n",
        "                            capital_gain, capital_loss, hours_per_week, native_country]],\n",
        "                          columns=['age', 'workclass', 'fnlwgt', 'education', 'educational-num',\n",
        "                                   'marital-status', 'occupation', 'relationship', 'race', 'gender',\n",
        "                                   'capital-gain', 'capital-loss', 'hours-per-week', 'native-country'])\n",
        "\n",
        "st.subheader(\"Your Input Details:\")\n",
        "st.write(input_data)\n",
        "\n",
        "# --- Preprocess input data ---\n",
        "# Apply the same cleaning as done during training\n",
        "input_data['workclass'] = input_data['workclass'].replace('?', 'Others')\n",
        "input_data['occupation'] = input_data['occupation'].replace('?', 'Others')\n",
        "\n",
        "# Remove 'Without-pay' or 'Never-worked' if somehow selected (though selectbox limits this)\n",
        "input_data = input_data[input_data['workclass'] != 'Without-pay']\n",
        "input_data = input_data[input_data['workclass'] != 'Never-worked']\n",
        "\n",
        "# --- IMPORTANT: Re-fit preprocessor to ensure all categories are known ---\n",
        "# This is a critical workaround if the preprocessor object wasn't saved/loaded.\n",
        "# It assumes the `data` DataFrame from the original script (after cleaning) is globally available.\n",
        "# In a real app.py, you'd define the `preprocessor_app` object with the categories it saw during training.\n",
        "# For now, let's create a *new* preprocessor and fit it on the input_data.\n",
        "# THIS IS NOT ROBUST FOR PRODUCTION. A robust solution saves/loads the fitted preprocessor.\n",
        "\n",
        "# A safer way would be to list all possible categories for OneHotEncoder,\n",
        "# or better, save and load the *fitted* preprocessor object.\n",
        "\n",
        "# For this demo, we'll recreate the preprocessor for the app and fit it on input data + dummy data\n",
        "# (or directly on the input if we are confident the categories are always known).\n",
        "\n",
        "# We MUST ensure `preprocessor_app` is properly fitted with *all* categories\n",
        "# that existed in the original training data to avoid `ValueError: Found unknown categories`.\n",
        "# A simple `ColumnTransformer` with `handle_unknown='ignore'` helps, but columns might be missing.\n",
        "\n",
        "# Let's attempt to define the preprocessor with the same `OneHotEncoder` settings as during training\n",
        "# And then transform the input.\n",
        "# The safest way is if you saved the `preprocessor` object from your main notebook.\n",
        "\n",
        "# For this specific app.py, without the saved preprocessor,\n",
        "# the OneHotEncoder needs to learn categories from *somewhere*.\n",
        "# We will make it handle unknown categories by 'ignore'.\n",
        "\n",
        "# Re-initialize the ColumnTransformer with the correct categorical columns\n",
        "# We assume 'educational-num', 'fnlwgt', 'age', 'capital-gain', 'capital-loss', 'hours-per-week' are numerical\n",
        "\n",
        "# Create a preprocessor like the one used in training.\n",
        "# IMPORTANT: In a real-world scenario, you would save the fitted `preprocessor`\n",
        "# from your training notebook and load it here (`joblib.load('preprocessor.joblib')`).\n",
        "# Since we didn't do that in the instructions, we have to re-create it.\n",
        "\n",
        "# The `OneHotEncoder` needs to be `fit` on all *possible* categories\n",
        "# to create the correct number of columns.\n",
        "# Without access to the original training data unique categories, this is a common challenge.\n",
        "\n",
        "# For a demo, we will define the `OneHotEncoder` with `handle_unknown='ignore'`\n",
        "# which will output all zeros for unseen categories.\n",
        "\n",
        "# Define the preprocessor again (should match the one in your main notebook)\n",
        "# Ensure numerical columns are correctly passed through\n",
        "numeric_features = ['age', 'fnlwgt', 'educational-num', 'capital-gain', '                     capital-loss', 'hours-per-week'] # Correct numerical columns\n",
        "\n",
        "# Ensure your `categorical_cols_app` list is accurate based on your original data\n",
        "\n",
        "# This is the best we can do without having saved the fitted preprocessor object.\n",
        "# It assumes the preprocessor will create columns correctly based on common input patterns.\n",
        "# For full robustness, save and load the actual fitted preprocessor.\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Define the preprocessor that will be used in the app\n",
        "# It must match the structure of the preprocessor used during model training\n",
        "# `handle_unknown='ignore'` is crucial for deployment to prevent errors if unseen categories appear\n",
        "# We assume 'educational-num', 'fnlwgt', 'age', 'capital-gain', 'capital-loss', 'hours-per-week' are numerical\n",
        "\n",
        "# First, list all columns in the exact order as they appear in `input_data` DataFrame\n",
        "all_input_cols = ['age', 'workclass', 'fnlwgt', 'education', 'educational-num',\n",
        "                  'marital-status', 'occupation', 'relationship', 'race', 'gender',\n",
        "                  'capital-gain', 'capital-loss', 'hours-per-week', 'native-country']\n",
        "\n",
        "# Now define the indices for categorical and numerical columns based on `all_input_cols`\n",
        "categorical_indices = [all_input_cols.index(col) for col in categorical_cols_app]\n",
        "numerical_indices = [i for i, col in enumerate(all_input_cols) if col not in categorical_cols_app]\n",
        "\n",
        "# Create the ColumnTransformer for the app\n",
        "# This version correctly handles the specific columns by their index/name\n",
        "preprocessor_for_app_transform = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols_app) # Use column names for easier mapping\n",
        "    ],\n",
        "    remainder='passthrough' # Pass through numerical columns\n",
        ")\n",
        "\n",
        "# IMPORTANT: The preprocessor_for_app_transform needs to be fitted on *all* categories\n",
        "# from the training data. Without saving the original preprocessor, this is a challenge.\n",
        "# For a robust deployment, `preprocessor` from your main notebook should be saved.\n",
        "# `joblib.dump(preprocessor, 'preprocessor.joblib')`\n",
        "# Then loaded here: `preprocessor_for_app_transform = joblib.load('preprocessor.joblib')`\n",
        "\n",
        "# Dummy fit to prevent errors if preprocessor was not saved (not robust for new categories)\n",
        "# This is a hacky way to ensure the preprocessor knows what columns to expect.\n",
        "# A robust solution involves saving the fitted preprocessor from training.\n",
        "# For now, we'll try to transform the input data directly, relying on 'handle_unknown=\"ignore\"'\n",
        "\n",
        "# Create an empty pipeline to apply just the preprocessor\n",
        "temp_pipeline = Pipeline(steps=[('preprocessor', preprocessor_for_app_transform)])\n",
        "\n",
        "# Fit this dummy pipeline on the input data to allow transformation.\n",
        "# This is not ideal as the categories learned are only from the *current* input.\n",
        "# A truly robust app loads the pre-fitted preprocessor.\n",
        "\n",
        "# The `fit` step for ColumnTransformer is crucial for OneHotEncoder to learn categories.\n",
        "# Without the original `X` data, we simulate it with the `input_data`.\n",
        "# This might fail if a category appears in test that wasn't in this `input_data`.\n",
        "# THE BEST WAY: save and load the trained `preprocessor` from your main notebook.\n",
        "\n",
        "# Workaround: manually specify known categories for OneHotEncoder if you don't save preprocessor\n",
        "# This would be a long list for each categorical column. Not practical for this quick demo.\n",
        "\n",
        "# Let's assume the order of `X` columns is maintained and use `fit_transform` directly on `input_data`\n",
        "# relying on `handle_unknown='ignore'`. This is less robust but gets the demo running.\n",
        "\n",
        "# Create the `preprocessor_for_app_transform` instance (same as what was used in training)\n",
        "preprocessor_for_app_transform = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols_app)\n",
        "    ],\n",
        "    remainder='passthrough' # Pass through numerical columns\n",
        ")\n",
        "\n",
        "# To avoid needing to fit on a dummy dataset, a very simple (but less robust) way is\n",
        "# to make the preprocessor ready to transform. This assumes the order of original X columns\n",
        "# and the number of categories are implicitly handled by `handle_unknown='ignore'` and column names.\n",
        "# For a real project, this is a major point of failure without saving the fitted preprocessor.\n",
        "\n",
        "# The `transform` method will work as long as the columns are in the expected order\n",
        "# and `handle_unknown='ignore'` is set.\n",
        "\n",
        "# Ensure the input data has the same column order as `X` used for training `preprocessor`\n",
        "# List all numerical columns for clarity\n",
        "num_cols = [col for col in input_data.columns if col not in categorical_cols_app]\n",
        "\n",
        "# Reorder input_data columns to match the training data order if necessary\n",
        "# (Assuming the original X columns had a specific order before one-hot encoding)\n",
        "\n",
        "# This is the line that will attempt to transform the single input row\n",
        "# It's crucial that `preprocessor_for_app_transform` is ready to do this.\n",
        "# We will fit it on a dummy DataFrame that has the exact structure and all possible categories.\n",
        "# This is often done by loading a small sample of the *original* training data.\n",
        "\n",
        "# Re-creating the ColumnTransformer and fitting it to an empty DataFrame with the correct column names\n",
        "# is a common hack if the fitted preprocessor object isn't saved.\n",
        "\n",
        "# Ensure preprocessor_for_app_transform is \"trained\" on the *structure* of your data\n",
        "# This part is the most critical for consistent deployment.\n",
        "# The best practice is to load the saved `preprocessor` object.\n",
        "# Since we didn't save it, let's create a *dummy* preprocessor that will learn based on the input columns.\n",
        "\n",
        "# One last attempt to make the preprocessor reliable without saving it:\n",
        "# Get all original columns (including numerical ones) from your dataset\n",
        "# You would need the exact original columns from your `data` DataFrame after initial cleaning.\n",
        "\n",
        "# Define a helper function to create and fit the preprocessor in the app\n",
        "# This is a common pattern for deployment when the preprocessor isn't saved.\n",
        "# It trains the preprocessor on the *expected* structure of the input data.\n",
        "def get_fitted_preprocessor(sample_df, cat_cols):\n",
        "    temp_preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
        "        ],\n",
        "        remainder='passthrough'\n",
        "    )\n",
        "    temp_preprocessor.fit(sample_df) # Fit on a sample to learn categories/columns\n",
        "    return temp_preprocessor\n",
        "\n",
        "# Create a small dummy DataFrame that has the structure of your input data\n",
        "# (i.e., the same columns as `input_data` DataFrame)\n",
        "# This `sample_for_fit` needs to contain ALL columns, both categorical and numerical.\n",
        "sample_for_fit = pd.DataFrame(columns=['age', 'workclass', 'fnlwgt', 'education', 'educational-num',\n",
        "                                        'marital-status', 'occupation', 'relationship', 'race', 'gender',\n",
        "                                        'capital-gain', 'capital-loss', 'hours-per-week', 'native-country'])\n",
        "\n",
        "# Append a row from `input_data` to ensure the columns are populated\n",
        "sample_for_fit = pd.concat([sample_for_fit, input_data])\n",
        "\n",
        "# Now, fit the preprocessor on this sample\n",
        "fitted_preprocessor_for_app = get_fitted_preprocessor(sample_for_fit, categorical_cols_app)\n",
        "\n",
        "# Now, use this fitted preprocessor to transform the actual input_data\n",
        "input_data_processed = fitted_preprocessor_for_app.transform(input_data)\n",
        "\n",
        "# --- Prediction Button ---\n",
        "if st.sidebar.button(\"Predict Salary Class\"):\n",
        "    if not input_data.empty: # Check if input_data is not empty after filtering\n",
        "        # Make sure the transformed input data has the same number of features as the training data\n",
        "        # This is a common debugging step.\n",
        "        # st.write(f\"Shape of processed input for prediction: {input_data_processed.shape}\")\n",
        "        # st.write(f\"Expected features from training: {knn_model.n_features_in_}\") # Requires sklearn >= 1.0\n",
        "\n",
        "        prediction_encoded = knn_model.predict(input_data_processed)\n",
        "\n",
        "        # Convert the numerical prediction back to original labels\n",
        "        # Remember your LabelEncoder 'le' from your training notebook?\n",
        "        # We need to manually map back or load 'le' if it was saved.\n",
        "        # Assuming 0 is <=50K and 1 is >50K based on typical LabelEncoder behavior\n",
        "        predicted_label = \"<=50K\" if prediction_encoded[0] == 0 else \">50K\"\n",
        "\n",
        "        st.success(f\"**Predicted Salary Class:** {predicted_label}\")\n",
        "    else:\n",
        "        st.warning(\"Please adjust input details. No valid data to predict.\")\n",
        "\n",
        "\n",
        "# --- Batch Prediction Feature ---\n",
        "st.markdown(\"#### 📂 Batch Prediction\")\n",
        "uploaded_file = st.file_uploader(\"Upload a CSV file for batch prediction\", type=[\"csv\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    batch_data = pd.read_csv(uploaded_file)\n",
        "    st.write(\"Uploaded data preview:\", batch_data.head())\n",
        "\n",
        "    # Apply the same cleaning as done for single input\n",
        "    batch_data['workclass'] = batch_data['workclass'].replace('?', 'Others')\n",
        "    batch_data['occupation'] = batch_data['occupation'].replace('?', 'Others')\n",
        "    batch_data = batch_data[batch_data['workclass'] != 'Without-pay']\n",
        "    batch_data = batch_data[batch_data['workclass'] != 'Never-worked']\n",
        "\n",
        "    # Ensure batch_data is not empty after cleaning\n",
        "    if not batch_data.empty:\n",
        "        # Transform batch data using the *same fitted preprocessor*\n",
        "        # This is crucial for consistency.\n",
        "        # We will refit the preprocessor on the *batch_data* just for this demo.\n",
        "        # In a real app, the `fitted_preprocessor_for_app` should be used directly.\n",
        "\n",
        "        # Using the same `fitted_preprocessor_for_app` from single prediction\n",
        "        batch_data_processed = fitted_preprocessor_for_app.transform(batch_data)\n",
        "\n",
        "        # Make predictions\n",
        "        batch_preds_encoded = knn_model.predict(batch_data_processed)\n",
        "\n",
        "        # Convert numerical predictions back to original labels\n",
        "        batch_preds_labels = [\"<=50K\" if p == 0 else \">50K\" for p in batch_preds_encoded]\n",
        "\n",
        "        batch_data['Predicted Income Class'] = batch_preds_labels\n",
        "\n",
        "        st.write(\"✅ Predictions:\")\n",
        "        st.write(batch_data.head()) # Show first few with predictions\n",
        "\n",
        "        csv = batch_data.to_csv(index=False).encode('utf-8')\n",
        "        st.download_button(\"Download Predictions CSV\", csv, file_name='predicted_salaries.csv', mime='text/csv')\n",
        "    else:\n",
        "        st.warning(\"No valid data found in the uploaded CSV after cleaning. Please check your file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nbCCnFHnSmH",
        "outputId": "34669ec5-fe2d-49e8-d992-b577925e4ccf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 🧪 Step 1: Install ngrok and streamlit (only once)\n",
        "!pip install pyngrok streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6BBv96sqVM3",
        "outputId": "5ccc1496-f7c4-4b41-b166-f77e7948846a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.12)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.47.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.47.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok --quiet"
      ],
      "metadata": {
        "id": "pSbE-40Rv7kP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import joblib\n",
        "\n",
        "st.title(\"Employee Salary Prediction\")\n",
        "\n",
        "st.write(\"This app predicts whether an employee earns more than 50K or not.\")\n",
        "\n",
        "age = st.slider(\"Age\", 18, 70, 30)\n",
        "education_num = st.slider(\"Education Level (numerical)\", 1, 16, 10)\n",
        "hours_per_week = st.slider(\"Hours per Week\", 1, 100, 40)\n",
        "capital_gain = st.number_input(\"Capital Gain\", 0, 100000, 0)\n",
        "capital_loss = st.number_input(\"Capital Loss\", 0, 100000, 0)\n",
        "\n",
        "if st.button(\"Predict\"):\n",
        "    try:\n",
        "        model = joblib.load(\"salary_model.pkl\")\n",
        "        data = [[age, education_num, hours_per_week, capital_gain, capital_loss]]\n",
        "        prediction = model.predict(data)\n",
        "        result = \"More than 50K\" if prediction[0] == 1 else \"50K or less\"\n",
        "        st.success(f\"Prediction: {result}\")\n",
        "    except:\n",
        "        st.error(\"Model not found. Please upload salary_model.pkl.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDnbV8gmwA-I",
        "outputId": "d1151008-3c92-4be9-b7e3-f83b4ac4c298"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Set ngrok token\n",
        "ngrok.set_auth_token(\"3099WCUCSXHbGszzxClHWlZyrjO_37B4JVosHkPRH81oimUcb\")\n",
        "\n",
        "# Start Streamlit in a separate thread\n",
        "def run():\n",
        "    os.system(\"streamlit run app.py --server.port 8501\")\n",
        "\n",
        "threading.Thread(target=run).start()\n",
        "\n",
        "# Wait for it to start\n",
        "time.sleep(5)\n",
        "\n",
        "# Create tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"🎉 Your Streamlit app is live at:\", public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WI_al9iRwFuD",
        "outputId": "1929d7cb-e294-4937-f224-6a6adb6642db"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎉 Your Streamlit app is live at: NgrokTunnel: \"https://7643904a66ce.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import joblib\n",
        "\n",
        "# Load the UCI Adult dataset (you can replace with your dataset)\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        "column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
        "                'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
        "                'hours-per-week', 'native-country', 'income']\n",
        "data = pd.read_csv(url, header=None, names=column_names, na_values=\" ?\", skipinitialspace=True)\n",
        "\n",
        "# Drop missing values\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Convert target to binary\n",
        "data['income'] = data['income'].apply(lambda x: 1 if x == ' >50K' else 0)\n",
        "\n",
        "# We'll use only numerical features for this basic model\n",
        "X = data[['age', 'education-num', 'hours-per-week', 'capital-gain', 'capital-loss']]\n",
        "y = data['income']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a simple model\n",
        "model = KNeighborsClassifier(n_neighbors=5)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save the model\n",
        "joblib.dump(model, 'salary_model.pkl')\n",
        "print(\"✅ Model trained and saved as salary_model.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9h5ngyPL0VwN",
        "outputId": "ef16b1e7-5856-4f6b-b5e8-af21847b2f64"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model trained and saved as salary_model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = joblib.load(\"salary_model.pkl\")"
      ],
      "metadata": {
        "id": "nN_j5Huu0wbF"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}
